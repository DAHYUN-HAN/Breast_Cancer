{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import optimizers\n",
    "#from tensorflow.keras.applications.vgg19 import VGG19, preprocess_input\n",
    "from tensorflow.keras.applications import InceptionV3\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler, Callback\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import RMSprop, SGD, Adam, Adadelta, Adagrad, Adamax, Nadam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import sklearn\n",
    "import datetime\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Activation, Flatten, BatchNormalization, GlobalAveragePooling2D  \n",
    "from tensorflow.keras.backend import batch_normalization\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from packaging import version\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training():\n",
    "    \"\"\"\n",
    "    Load the training set (excluding baseline patches)\n",
    "    \"\"\"\n",
    "    images = np.load(os.path.join('Data_again', 'X_train.npy'))\n",
    "    labels = np.load(os.path.join('Data_again', 'train_labels_multi.npy'))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def load_testing():\n",
    "    \"\"\"\n",
    "    Load the test set (abnormalities patches and labels, no baseline)\n",
    "    \"\"\"\n",
    "    images = np.load(os.path.join('Data_again', 'X_test.npy'))\n",
    "    labels = np.load(os.path.join('Data_again', 'y_test_labels_multi.npy'))\n",
    "    return images, labels\n",
    "\n",
    "\n",
    "def remap_label(l):\n",
    "    \"\"\"\n",
    "    Remap the labels to:\n",
    "        0 -> mass benign \n",
    "        1 -> mass malignant\n",
    "        2 -> calcification benign\n",
    "        3 -> calcification malignant\n",
    "    \"\"\"\n",
    "    if 1 <= l <= 4:\n",
    "        return l-1\n",
    "    else:\n",
    "        print(\"[WARN] Unrecognized label (%d)\" % l)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2913 \t Test size: 655\n",
      "Image size: 256x256\n"
     ]
    }
   ],
   "source": [
    "# Load training and test images (abnormalities only, no baseline)\n",
    "train_images, train_labels= load_training()\n",
    "test_images, test_labels= load_testing()\n",
    "\n",
    "# Number of images\n",
    "n_train_img = train_images.shape[0]\n",
    "n_test_img = test_images.shape[0]\n",
    "print(\"Train size: %d \\t Test size: %d\" % (n_train_img, n_test_img))\n",
    "\n",
    "# Compute width and height of images\n",
    "img_w = train_images.shape[1]\n",
    "img_h = train_images.shape[2]\n",
    "print(\"Image size: %dx%d\" % (img_w, img_h))\n",
    "\n",
    "# Convert the labels to categorical format\n",
    "train_labels = to_categorical(train_labels)\n",
    "test_labels_raw = test_labels.copy()\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "# Create a new dimension for color in the images arrays\n",
    "train_images = train_images.reshape((n_train_img, img_w, img_h, 1))\n",
    "test_images = test_images.reshape((n_test_img, img_w, img_h, 1))\n",
    "\n",
    "# Convert from 16-bit (0-65535) to to 8-bit (0-255)\n",
    "train_images = train_images.astype('uint16') / 255\n",
    "test_images = test_images.astype('uint16') / 255\n",
    "\n",
    "# Replicate the only color channel (gray) 3 times, for VGGNet compatibility\n",
    "train_images = np.repeat(train_images, 3, axis=3)\n",
    "test_images = np.repeat(test_images, 3, axis=3)\n",
    "\n",
    "# Shuffle the training set (originally sorted by label)\n",
    "perm = np.random.permutation(n_train_img)\n",
    "train_images = train_images[perm]\n",
    "train_labels = train_labels[perm]\n",
    "\n",
    "# Create a generator for training images\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=180,\n",
    "    shear_range=15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='reflect'\n",
    ")\n",
    "\n",
    "# Fit the generator with some images\n",
    "train_datagen.fit(train_images)\n",
    "\n",
    "# Split train images into actual training and validation\n",
    "train_generator = train_datagen.flow(train_images, train_labels, batch_size=128, subset='training')\n",
    "validation_generator = train_datagen.flow(train_images, train_labels, batch_size=128, subset='validation')\n",
    "\n",
    "# Preprocess the test images as well\n",
    "preprocess_input(test_images);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a model using VGG16 convolutional base and new FC final layer\n",
    "\n",
    "def create_inceptionv3(verbose=False, fc_size=256, dropout=None):\n",
    "    \n",
    "    inceptionv3_base = InceptionV3(weights='imagenet',\n",
    "                       include_top=False,\n",
    "                       input_shape=(256, 256, 3))\n",
    "    inceptionv3 = models.Sequential()\n",
    "    inceptionv3.add(inceptionv3_base)\n",
    "\n",
    "    inceptionv3.add(layers.Flatten())\n",
    "    if dropout is not None:\n",
    "        inceptionv3.add(layers.Dropout(dropout))\n",
    "    inceptionv3.add(layers.Dense(2048, activation='relu'))\n",
    "    inceptionv3.add(layers.Dense(1048, activation='relu'))\n",
    "    inceptionv3.add(layers.Dense(fc_size, activation='relu'))\n",
    "    inceptionv3.add(layers.Dense(4, activation='softmax'))\n",
    "\n",
    "    # Freeze the convolutional base\n",
    "    inceptionv3_base.trainable = False\n",
    "    \n",
    "    if verbose:\n",
    "        inceptionv3_base.summary()\n",
    "        inceptionv3.summary()\n",
    "\n",
    "    return inceptionv3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inceptionv3_fe_drop_temp = create_inceptionv3(verbose=True, dropout=0.5, fc_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"inception_v3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 256, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 127, 127, 32) 864         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 127, 127, 32) 96          conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_94 (Activation)      (None, 127, 127, 32) 0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_95 (Conv2D)              (None, 125, 125, 32) 9216        activation_94[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_95 (BatchNo (None, 125, 125, 32) 96          conv2d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_95 (Activation)      (None, 125, 125, 32) 0           batch_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_96 (Conv2D)              (None, 125, 125, 64) 18432       activation_95[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_96 (BatchNo (None, 125, 125, 64) 192         conv2d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_96 (Activation)      (None, 125, 125, 64) 0           batch_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 62, 62, 64)   0           activation_96[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_97 (Conv2D)              (None, 62, 62, 80)   5120        max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_97 (BatchNo (None, 62, 62, 80)   240         conv2d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_97 (Activation)      (None, 62, 62, 80)   0           batch_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_98 (Conv2D)              (None, 60, 60, 192)  138240      activation_97[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_98 (BatchNo (None, 60, 60, 192)  576         conv2d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_98 (Activation)      (None, 60, 60, 192)  0           batch_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 29, 29, 192)  0           activation_98[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_102 (Conv2D)             (None, 29, 29, 64)   12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_102 (BatchN (None, 29, 29, 64)   192         conv2d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_102 (Activation)     (None, 29, 29, 64)   0           batch_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_100 (Conv2D)             (None, 29, 29, 48)   9216        max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_103 (Conv2D)             (None, 29, 29, 96)   55296       activation_102[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_100 (BatchN (None, 29, 29, 48)   144         conv2d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_103 (BatchN (None, 29, 29, 96)   288         conv2d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_100 (Activation)     (None, 29, 29, 48)   0           batch_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_103 (Activation)     (None, 29, 29, 96)   0           batch_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_9 (AveragePoo (None, 29, 29, 192)  0           max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_99 (Conv2D)              (None, 29, 29, 64)   12288       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_101 (Conv2D)             (None, 29, 29, 64)   76800       activation_100[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_104 (Conv2D)             (None, 29, 29, 96)   82944       activation_103[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_105 (Conv2D)             (None, 29, 29, 32)   6144        average_pooling2d_9[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_99 (BatchNo (None, 29, 29, 64)   192         conv2d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_101 (BatchN (None, 29, 29, 64)   192         conv2d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_104 (BatchN (None, 29, 29, 96)   288         conv2d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_105 (BatchN (None, 29, 29, 32)   96          conv2d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_99 (Activation)      (None, 29, 29, 64)   0           batch_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "activation_101 (Activation)     (None, 29, 29, 64)   0           batch_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_104 (Activation)     (None, 29, 29, 96)   0           batch_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_105 (Activation)     (None, 29, 29, 32)   0           batch_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed0 (Concatenate)            (None, 29, 29, 256)  0           activation_99[0][0]              \n",
      "                                                                 activation_101[0][0]             \n",
      "                                                                 activation_104[0][0]             \n",
      "                                                                 activation_105[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_109 (Conv2D)             (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_109 (BatchN (None, 29, 29, 64)   192         conv2d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_109 (Activation)     (None, 29, 29, 64)   0           batch_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_107 (Conv2D)             (None, 29, 29, 48)   12288       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_110 (Conv2D)             (None, 29, 29, 96)   55296       activation_109[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_107 (BatchN (None, 29, 29, 48)   144         conv2d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_110 (BatchN (None, 29, 29, 96)   288         conv2d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_107 (Activation)     (None, 29, 29, 48)   0           batch_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_110 (Activation)     (None, 29, 29, 96)   0           batch_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_10 (AveragePo (None, 29, 29, 256)  0           mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_106 (Conv2D)             (None, 29, 29, 64)   16384       mixed0[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_108 (Conv2D)             (None, 29, 29, 64)   76800       activation_107[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_111 (Conv2D)             (None, 29, 29, 96)   82944       activation_110[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_112 (Conv2D)             (None, 29, 29, 64)   16384       average_pooling2d_10[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_106 (BatchN (None, 29, 29, 64)   192         conv2d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_108 (BatchN (None, 29, 29, 64)   192         conv2d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_111 (BatchN (None, 29, 29, 96)   288         conv2d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_112 (BatchN (None, 29, 29, 64)   192         conv2d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_106 (Activation)     (None, 29, 29, 64)   0           batch_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_108 (Activation)     (None, 29, 29, 64)   0           batch_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_111 (Activation)     (None, 29, 29, 96)   0           batch_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_112 (Activation)     (None, 29, 29, 64)   0           batch_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed1 (Concatenate)            (None, 29, 29, 288)  0           activation_106[0][0]             \n",
      "                                                                 activation_108[0][0]             \n",
      "                                                                 activation_111[0][0]             \n",
      "                                                                 activation_112[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_116 (Conv2D)             (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_116 (BatchN (None, 29, 29, 64)   192         conv2d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_116 (Activation)     (None, 29, 29, 64)   0           batch_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_114 (Conv2D)             (None, 29, 29, 48)   13824       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_117 (Conv2D)             (None, 29, 29, 96)   55296       activation_116[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_114 (BatchN (None, 29, 29, 48)   144         conv2d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_117 (BatchN (None, 29, 29, 96)   288         conv2d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_114 (Activation)     (None, 29, 29, 48)   0           batch_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_117 (Activation)     (None, 29, 29, 96)   0           batch_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_11 (AveragePo (None, 29, 29, 288)  0           mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_113 (Conv2D)             (None, 29, 29, 64)   18432       mixed1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_115 (Conv2D)             (None, 29, 29, 64)   76800       activation_114[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_118 (Conv2D)             (None, 29, 29, 96)   82944       activation_117[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_119 (Conv2D)             (None, 29, 29, 64)   18432       average_pooling2d_11[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_113 (BatchN (None, 29, 29, 64)   192         conv2d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_115 (BatchN (None, 29, 29, 64)   192         conv2d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_118 (BatchN (None, 29, 29, 96)   288         conv2d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_119 (BatchN (None, 29, 29, 64)   192         conv2d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_113 (Activation)     (None, 29, 29, 64)   0           batch_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_115 (Activation)     (None, 29, 29, 64)   0           batch_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_118 (Activation)     (None, 29, 29, 96)   0           batch_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_119 (Activation)     (None, 29, 29, 64)   0           batch_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed2 (Concatenate)            (None, 29, 29, 288)  0           activation_113[0][0]             \n",
      "                                                                 activation_115[0][0]             \n",
      "                                                                 activation_118[0][0]             \n",
      "                                                                 activation_119[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_121 (Conv2D)             (None, 29, 29, 64)   18432       mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_121 (BatchN (None, 29, 29, 64)   192         conv2d_121[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_121 (Activation)     (None, 29, 29, 64)   0           batch_normalization_121[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_122 (Conv2D)             (None, 29, 29, 96)   55296       activation_121[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_122 (BatchN (None, 29, 29, 96)   288         conv2d_122[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_122 (Activation)     (None, 29, 29, 96)   0           batch_normalization_122[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_120 (Conv2D)             (None, 14, 14, 384)  995328      mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_123 (Conv2D)             (None, 14, 14, 96)   82944       activation_122[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_120 (BatchN (None, 14, 14, 384)  1152        conv2d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_123 (BatchN (None, 14, 14, 96)   288         conv2d_123[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_120 (Activation)     (None, 14, 14, 384)  0           batch_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_123 (Activation)     (None, 14, 14, 96)   0           batch_normalization_123[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 14, 14, 288)  0           mixed2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed3 (Concatenate)            (None, 14, 14, 768)  0           activation_120[0][0]             \n",
      "                                                                 activation_123[0][0]             \n",
      "                                                                 max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_128 (Conv2D)             (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_128 (BatchN (None, 14, 14, 128)  384         conv2d_128[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_128 (Activation)     (None, 14, 14, 128)  0           batch_normalization_128[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_129 (Conv2D)             (None, 14, 14, 128)  114688      activation_128[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_129 (BatchN (None, 14, 14, 128)  384         conv2d_129[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_129 (Activation)     (None, 14, 14, 128)  0           batch_normalization_129[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_125 (Conv2D)             (None, 14, 14, 128)  98304       mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_130 (Conv2D)             (None, 14, 14, 128)  114688      activation_129[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_125 (BatchN (None, 14, 14, 128)  384         conv2d_125[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_130 (BatchN (None, 14, 14, 128)  384         conv2d_130[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_125 (Activation)     (None, 14, 14, 128)  0           batch_normalization_125[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_130 (Activation)     (None, 14, 14, 128)  0           batch_normalization_130[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_126 (Conv2D)             (None, 14, 14, 128)  114688      activation_125[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_131 (Conv2D)             (None, 14, 14, 128)  114688      activation_130[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_126 (BatchN (None, 14, 14, 128)  384         conv2d_126[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_131 (BatchN (None, 14, 14, 128)  384         conv2d_131[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_126 (Activation)     (None, 14, 14, 128)  0           batch_normalization_126[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_131 (Activation)     (None, 14, 14, 128)  0           batch_normalization_131[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_12 (AveragePo (None, 14, 14, 768)  0           mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_124 (Conv2D)             (None, 14, 14, 192)  147456      mixed3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_127 (Conv2D)             (None, 14, 14, 192)  172032      activation_126[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_132 (Conv2D)             (None, 14, 14, 192)  172032      activation_131[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_133 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_12[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_124 (BatchN (None, 14, 14, 192)  576         conv2d_124[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_127 (BatchN (None, 14, 14, 192)  576         conv2d_127[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_132 (BatchN (None, 14, 14, 192)  576         conv2d_132[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_133 (BatchN (None, 14, 14, 192)  576         conv2d_133[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_124 (Activation)     (None, 14, 14, 192)  0           batch_normalization_124[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_127 (Activation)     (None, 14, 14, 192)  0           batch_normalization_127[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_132 (Activation)     (None, 14, 14, 192)  0           batch_normalization_132[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_133 (Activation)     (None, 14, 14, 192)  0           batch_normalization_133[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed4 (Concatenate)            (None, 14, 14, 768)  0           activation_124[0][0]             \n",
      "                                                                 activation_127[0][0]             \n",
      "                                                                 activation_132[0][0]             \n",
      "                                                                 activation_133[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_138 (Conv2D)             (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_138 (BatchN (None, 14, 14, 160)  480         conv2d_138[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_138 (Activation)     (None, 14, 14, 160)  0           batch_normalization_138[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_139 (Conv2D)             (None, 14, 14, 160)  179200      activation_138[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_139 (BatchN (None, 14, 14, 160)  480         conv2d_139[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_139 (Activation)     (None, 14, 14, 160)  0           batch_normalization_139[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_135 (Conv2D)             (None, 14, 14, 160)  122880      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_140 (Conv2D)             (None, 14, 14, 160)  179200      activation_139[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_135 (BatchN (None, 14, 14, 160)  480         conv2d_135[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_140 (BatchN (None, 14, 14, 160)  480         conv2d_140[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_135 (Activation)     (None, 14, 14, 160)  0           batch_normalization_135[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_140 (Activation)     (None, 14, 14, 160)  0           batch_normalization_140[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_136 (Conv2D)             (None, 14, 14, 160)  179200      activation_135[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_141 (Conv2D)             (None, 14, 14, 160)  179200      activation_140[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_136 (BatchN (None, 14, 14, 160)  480         conv2d_136[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_141 (BatchN (None, 14, 14, 160)  480         conv2d_141[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_136 (Activation)     (None, 14, 14, 160)  0           batch_normalization_136[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_141 (Activation)     (None, 14, 14, 160)  0           batch_normalization_141[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_13 (AveragePo (None, 14, 14, 768)  0           mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_134 (Conv2D)             (None, 14, 14, 192)  147456      mixed4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_137 (Conv2D)             (None, 14, 14, 192)  215040      activation_136[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_142 (Conv2D)             (None, 14, 14, 192)  215040      activation_141[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_143 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_13[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_134 (BatchN (None, 14, 14, 192)  576         conv2d_134[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_137 (BatchN (None, 14, 14, 192)  576         conv2d_137[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_142 (BatchN (None, 14, 14, 192)  576         conv2d_142[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_143 (BatchN (None, 14, 14, 192)  576         conv2d_143[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_134 (Activation)     (None, 14, 14, 192)  0           batch_normalization_134[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_137 (Activation)     (None, 14, 14, 192)  0           batch_normalization_137[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_142 (Activation)     (None, 14, 14, 192)  0           batch_normalization_142[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_143 (Activation)     (None, 14, 14, 192)  0           batch_normalization_143[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed5 (Concatenate)            (None, 14, 14, 768)  0           activation_134[0][0]             \n",
      "                                                                 activation_137[0][0]             \n",
      "                                                                 activation_142[0][0]             \n",
      "                                                                 activation_143[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_148 (Conv2D)             (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_148 (BatchN (None, 14, 14, 160)  480         conv2d_148[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_148 (Activation)     (None, 14, 14, 160)  0           batch_normalization_148[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_149 (Conv2D)             (None, 14, 14, 160)  179200      activation_148[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_149 (BatchN (None, 14, 14, 160)  480         conv2d_149[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_149 (Activation)     (None, 14, 14, 160)  0           batch_normalization_149[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_145 (Conv2D)             (None, 14, 14, 160)  122880      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_150 (Conv2D)             (None, 14, 14, 160)  179200      activation_149[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_145 (BatchN (None, 14, 14, 160)  480         conv2d_145[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_150 (BatchN (None, 14, 14, 160)  480         conv2d_150[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_145 (Activation)     (None, 14, 14, 160)  0           batch_normalization_145[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_150 (Activation)     (None, 14, 14, 160)  0           batch_normalization_150[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_146 (Conv2D)             (None, 14, 14, 160)  179200      activation_145[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_151 (Conv2D)             (None, 14, 14, 160)  179200      activation_150[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_146 (BatchN (None, 14, 14, 160)  480         conv2d_146[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_151 (BatchN (None, 14, 14, 160)  480         conv2d_151[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_146 (Activation)     (None, 14, 14, 160)  0           batch_normalization_146[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_151 (Activation)     (None, 14, 14, 160)  0           batch_normalization_151[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_14 (AveragePo (None, 14, 14, 768)  0           mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_144 (Conv2D)             (None, 14, 14, 192)  147456      mixed5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_147 (Conv2D)             (None, 14, 14, 192)  215040      activation_146[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_152 (Conv2D)             (None, 14, 14, 192)  215040      activation_151[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_153 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_14[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_144 (BatchN (None, 14, 14, 192)  576         conv2d_144[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_147 (BatchN (None, 14, 14, 192)  576         conv2d_147[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_152 (BatchN (None, 14, 14, 192)  576         conv2d_152[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_153 (BatchN (None, 14, 14, 192)  576         conv2d_153[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_144 (Activation)     (None, 14, 14, 192)  0           batch_normalization_144[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_147 (Activation)     (None, 14, 14, 192)  0           batch_normalization_147[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_152 (Activation)     (None, 14, 14, 192)  0           batch_normalization_152[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_153 (Activation)     (None, 14, 14, 192)  0           batch_normalization_153[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed6 (Concatenate)            (None, 14, 14, 768)  0           activation_144[0][0]             \n",
      "                                                                 activation_147[0][0]             \n",
      "                                                                 activation_152[0][0]             \n",
      "                                                                 activation_153[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_158 (Conv2D)             (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_158 (BatchN (None, 14, 14, 192)  576         conv2d_158[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_158 (Activation)     (None, 14, 14, 192)  0           batch_normalization_158[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_159 (Conv2D)             (None, 14, 14, 192)  258048      activation_158[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_159 (BatchN (None, 14, 14, 192)  576         conv2d_159[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_159 (Activation)     (None, 14, 14, 192)  0           batch_normalization_159[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_155 (Conv2D)             (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_160 (Conv2D)             (None, 14, 14, 192)  258048      activation_159[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_155 (BatchN (None, 14, 14, 192)  576         conv2d_155[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_160 (BatchN (None, 14, 14, 192)  576         conv2d_160[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_155 (Activation)     (None, 14, 14, 192)  0           batch_normalization_155[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_160 (Activation)     (None, 14, 14, 192)  0           batch_normalization_160[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_156 (Conv2D)             (None, 14, 14, 192)  258048      activation_155[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_161 (Conv2D)             (None, 14, 14, 192)  258048      activation_160[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_156 (BatchN (None, 14, 14, 192)  576         conv2d_156[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_161 (BatchN (None, 14, 14, 192)  576         conv2d_161[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_156 (Activation)     (None, 14, 14, 192)  0           batch_normalization_156[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_161 (Activation)     (None, 14, 14, 192)  0           batch_normalization_161[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_15 (AveragePo (None, 14, 14, 768)  0           mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_154 (Conv2D)             (None, 14, 14, 192)  147456      mixed6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_157 (Conv2D)             (None, 14, 14, 192)  258048      activation_156[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_162 (Conv2D)             (None, 14, 14, 192)  258048      activation_161[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_163 (Conv2D)             (None, 14, 14, 192)  147456      average_pooling2d_15[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_154 (BatchN (None, 14, 14, 192)  576         conv2d_154[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_157 (BatchN (None, 14, 14, 192)  576         conv2d_157[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_162 (BatchN (None, 14, 14, 192)  576         conv2d_162[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_163 (BatchN (None, 14, 14, 192)  576         conv2d_163[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_154 (Activation)     (None, 14, 14, 192)  0           batch_normalization_154[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_157 (Activation)     (None, 14, 14, 192)  0           batch_normalization_157[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_162 (Activation)     (None, 14, 14, 192)  0           batch_normalization_162[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_163 (Activation)     (None, 14, 14, 192)  0           batch_normalization_163[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed7 (Concatenate)            (None, 14, 14, 768)  0           activation_154[0][0]             \n",
      "                                                                 activation_157[0][0]             \n",
      "                                                                 activation_162[0][0]             \n",
      "                                                                 activation_163[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_166 (Conv2D)             (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_166 (BatchN (None, 14, 14, 192)  576         conv2d_166[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_166 (Activation)     (None, 14, 14, 192)  0           batch_normalization_166[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_167 (Conv2D)             (None, 14, 14, 192)  258048      activation_166[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_167 (BatchN (None, 14, 14, 192)  576         conv2d_167[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_167 (Activation)     (None, 14, 14, 192)  0           batch_normalization_167[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_164 (Conv2D)             (None, 14, 14, 192)  147456      mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_168 (Conv2D)             (None, 14, 14, 192)  258048      activation_167[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_164 (BatchN (None, 14, 14, 192)  576         conv2d_164[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_168 (BatchN (None, 14, 14, 192)  576         conv2d_168[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_164 (Activation)     (None, 14, 14, 192)  0           batch_normalization_164[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_168 (Activation)     (None, 14, 14, 192)  0           batch_normalization_168[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_165 (Conv2D)             (None, 6, 6, 320)    552960      activation_164[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_169 (Conv2D)             (None, 6, 6, 192)    331776      activation_168[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_165 (BatchN (None, 6, 6, 320)    960         conv2d_165[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_169 (BatchN (None, 6, 6, 192)    576         conv2d_169[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_165 (Activation)     (None, 6, 6, 320)    0           batch_normalization_165[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_169 (Activation)     (None, 6, 6, 192)    0           batch_normalization_169[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 768)    0           mixed7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "mixed8 (Concatenate)            (None, 6, 6, 1280)   0           activation_165[0][0]             \n",
      "                                                                 activation_169[0][0]             \n",
      "                                                                 max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_174 (Conv2D)             (None, 6, 6, 448)    573440      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_174 (BatchN (None, 6, 6, 448)    1344        conv2d_174[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_174 (Activation)     (None, 6, 6, 448)    0           batch_normalization_174[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_171 (Conv2D)             (None, 6, 6, 384)    491520      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_175 (Conv2D)             (None, 6, 6, 384)    1548288     activation_174[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_171 (BatchN (None, 6, 6, 384)    1152        conv2d_171[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_175 (BatchN (None, 6, 6, 384)    1152        conv2d_175[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_171 (Activation)     (None, 6, 6, 384)    0           batch_normalization_171[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_175 (Activation)     (None, 6, 6, 384)    0           batch_normalization_175[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_172 (Conv2D)             (None, 6, 6, 384)    442368      activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_173 (Conv2D)             (None, 6, 6, 384)    442368      activation_171[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_176 (Conv2D)             (None, 6, 6, 384)    442368      activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_177 (Conv2D)             (None, 6, 6, 384)    442368      activation_175[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_16 (AveragePo (None, 6, 6, 1280)   0           mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_170 (Conv2D)             (None, 6, 6, 320)    409600      mixed8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_172 (BatchN (None, 6, 6, 384)    1152        conv2d_172[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_173 (BatchN (None, 6, 6, 384)    1152        conv2d_173[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_176 (BatchN (None, 6, 6, 384)    1152        conv2d_176[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_177 (BatchN (None, 6, 6, 384)    1152        conv2d_177[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_178 (Conv2D)             (None, 6, 6, 192)    245760      average_pooling2d_16[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_170 (BatchN (None, 6, 6, 320)    960         conv2d_170[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_172 (Activation)     (None, 6, 6, 384)    0           batch_normalization_172[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_173 (Activation)     (None, 6, 6, 384)    0           batch_normalization_173[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_176 (Activation)     (None, 6, 6, 384)    0           batch_normalization_176[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_177 (Activation)     (None, 6, 6, 384)    0           batch_normalization_177[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_178 (BatchN (None, 6, 6, 192)    576         conv2d_178[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_170 (Activation)     (None, 6, 6, 320)    0           batch_normalization_170[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_0 (Concatenate)          (None, 6, 6, 768)    0           activation_172[0][0]             \n",
      "                                                                 activation_173[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 6, 6, 768)    0           activation_176[0][0]             \n",
      "                                                                 activation_177[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_178 (Activation)     (None, 6, 6, 192)    0           batch_normalization_178[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9 (Concatenate)            (None, 6, 6, 2048)   0           activation_170[0][0]             \n",
      "                                                                 mixed9_0[0][0]                   \n",
      "                                                                 concatenate_2[0][0]              \n",
      "                                                                 activation_178[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_183 (Conv2D)             (None, 6, 6, 448)    917504      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_183 (BatchN (None, 6, 6, 448)    1344        conv2d_183[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_183 (Activation)     (None, 6, 6, 448)    0           batch_normalization_183[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_180 (Conv2D)             (None, 6, 6, 384)    786432      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_184 (Conv2D)             (None, 6, 6, 384)    1548288     activation_183[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_180 (BatchN (None, 6, 6, 384)    1152        conv2d_180[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_184 (BatchN (None, 6, 6, 384)    1152        conv2d_184[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_180 (Activation)     (None, 6, 6, 384)    0           batch_normalization_180[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_184 (Activation)     (None, 6, 6, 384)    0           batch_normalization_184[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_181 (Conv2D)             (None, 6, 6, 384)    442368      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_182 (Conv2D)             (None, 6, 6, 384)    442368      activation_180[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_185 (Conv2D)             (None, 6, 6, 384)    442368      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_186 (Conv2D)             (None, 6, 6, 384)    442368      activation_184[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_17 (AveragePo (None, 6, 6, 2048)   0           mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_179 (Conv2D)             (None, 6, 6, 320)    655360      mixed9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_181 (BatchN (None, 6, 6, 384)    1152        conv2d_181[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_182 (BatchN (None, 6, 6, 384)    1152        conv2d_182[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_185 (BatchN (None, 6, 6, 384)    1152        conv2d_185[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_186 (BatchN (None, 6, 6, 384)    1152        conv2d_186[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_187 (Conv2D)             (None, 6, 6, 192)    393216      average_pooling2d_17[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_179 (BatchN (None, 6, 6, 320)    960         conv2d_179[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_181 (Activation)     (None, 6, 6, 384)    0           batch_normalization_181[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_182 (Activation)     (None, 6, 6, 384)    0           batch_normalization_182[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_185 (Activation)     (None, 6, 6, 384)    0           batch_normalization_185[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "activation_186 (Activation)     (None, 6, 6, 384)    0           batch_normalization_186[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_187 (BatchN (None, 6, 6, 192)    576         conv2d_187[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "activation_179 (Activation)     (None, 6, 6, 320)    0           batch_normalization_179[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed9_1 (Concatenate)          (None, 6, 6, 768)    0           activation_181[0][0]             \n",
      "                                                                 activation_182[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 6, 6, 768)    0           activation_185[0][0]             \n",
      "                                                                 activation_186[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_187 (Activation)     (None, 6, 6, 192)    0           batch_normalization_187[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "mixed10 (Concatenate)           (None, 6, 6, 2048)   0           activation_179[0][0]             \n",
      "                                                                 mixed9_1[0][0]                   \n",
      "                                                                 concatenate_3[0][0]              \n",
      "                                                                 activation_187[0][0]             \n",
      "==================================================================================================\n",
      "Total params: 21,802,784\n",
      "Trainable params: 0\n",
      "Non-trainable params: 21,802,784\n",
      "__________________________________________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 6, 6, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               9437312   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 31,240,612\n",
      "Trainable params: 9,437,828\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate a VGG19 network with custom final layer\n",
    "inceptionv3_fe_drop_128 = create_inceptionv3(verbose=True, dropout=0.5, fc_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "inception_v3 (Model)         (None, 6, 6, 2048)        21802784  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 73728)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 128)               9437312   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 31,240,612\n",
      "Trainable params: 9,437,828\n",
      "Non-trainable params: 21,802,784\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inceptionv3_fe_drop_128.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping (stop training after the validation loss reaches the minimum)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=1)\n",
    "\n",
    "# Callback for checkpointing\n",
    "checkpoint = ModelCheckpoint('inceptionv3_fe_drop_128_4cl_best.h5', \n",
    "        monitor='val_loss', mode='min', verbose=1, \n",
    "        save_best_only=True, save_freq='epoch'\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "inceptionv3_fe_drop_128.compile(optimizer=SGD(lr=0.00001, momentum=0.9),loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 19 steps, validate for 5 steps\n",
      "Epoch 1/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.6477 - accuracy: 0.2910\n",
      "Epoch 00001: val_loss improved from inf to 11.82646, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 34s 2s/step - loss: 1.6445 - accuracy: 0.2917 - val_loss: 11.8265 - val_accuracy: 0.2079\n",
      "Epoch 2/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.6613 - accuracy: 0.2873\n",
      "Epoch 00002: val_loss did not improve from 11.82646\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.6606 - accuracy: 0.2896 - val_loss: 12.9837 - val_accuracy: 0.2182\n",
      "Epoch 3/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.6149 - accuracy: 0.2823\n",
      "Epoch 00003: val_loss improved from 11.82646 to 11.46210, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.6216 - accuracy: 0.2831 - val_loss: 11.4621 - val_accuracy: 0.2079\n",
      "Epoch 4/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5845 - accuracy: 0.3069\n",
      "Epoch 00004: val_loss did not improve from 11.46210\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5822 - accuracy: 0.3076 - val_loss: 11.4812 - val_accuracy: 0.2131\n",
      "Epoch 5/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5900 - accuracy: 0.3159\n",
      "Epoch 00005: val_loss improved from 11.46210 to 10.87279, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5904 - accuracy: 0.3140 - val_loss: 10.8728 - val_accuracy: 0.1873\n",
      "Epoch 6/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5569 - accuracy: 0.3164\n",
      "Epoch 00006: val_loss improved from 10.87279 to 10.73543, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5590 - accuracy: 0.3175 - val_loss: 10.7354 - val_accuracy: 0.2096\n",
      "Epoch 7/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5433 - accuracy: 0.3232\n",
      "Epoch 00007: val_loss improved from 10.73543 to 10.45175, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5477 - accuracy: 0.3218 - val_loss: 10.4517 - val_accuracy: 0.2010\n",
      "Epoch 8/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5489 - accuracy: 0.3096\n",
      "Epoch 00008: val_loss did not improve from 10.45175\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.5520 - accuracy: 0.3093 - val_loss: 10.4900 - val_accuracy: 0.1976\n",
      "Epoch 9/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5323 - accuracy: 0.3318\n",
      "Epoch 00009: val_loss improved from 10.45175 to 10.38498, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5251 - accuracy: 0.3346 - val_loss: 10.3850 - val_accuracy: 0.2131\n",
      "Epoch 10/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5475 - accuracy: 0.3264\n",
      "Epoch 00010: val_loss improved from 10.38498 to 10.00115, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5422 - accuracy: 0.3290 - val_loss: 10.0012 - val_accuracy: 0.2062\n",
      "Epoch 11/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5355 - accuracy: 0.3277\n",
      "Epoch 00011: val_loss did not improve from 10.00115\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.5294 - accuracy: 0.3290 - val_loss: 10.7952 - val_accuracy: 0.1976\n",
      "Epoch 12/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.5243 - accuracy: 0.3359\n",
      "Epoch 00012: val_loss improved from 10.00115 to 9.54769, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.5306 - accuracy: 0.3346 - val_loss: 9.5477 - val_accuracy: 0.2165\n",
      "Epoch 13/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4933 - accuracy: 0.3364\n",
      "Epoch 00013: val_loss did not improve from 9.54769\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4925 - accuracy: 0.3359 - val_loss: 9.6474 - val_accuracy: 0.1976\n",
      "Epoch 14/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4790 - accuracy: 0.3572\n",
      "Epoch 00014: val_loss improved from 9.54769 to 9.25561, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4837 - accuracy: 0.3531 - val_loss: 9.2556 - val_accuracy: 0.2234\n",
      "Epoch 15/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4789 - accuracy: 0.3350\n",
      "Epoch 00015: val_loss did not improve from 9.25561\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.4794 - accuracy: 0.3372 - val_loss: 9.7455 - val_accuracy: 0.2113\n",
      "Epoch 16/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4488 - accuracy: 0.3486\n",
      "Epoch 00016: val_loss improved from 9.25561 to 9.23005, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4473 - accuracy: 0.3488 - val_loss: 9.2301 - val_accuracy: 0.2165\n",
      "Epoch 17/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4798 - accuracy: 0.3482\n",
      "Epoch 00017: val_loss did not improve from 9.23005\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.4793 - accuracy: 0.3492 - val_loss: 9.6399 - val_accuracy: 0.2182\n",
      "Epoch 18/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4483 - accuracy: 0.3622\n",
      "Epoch 00018: val_loss improved from 9.23005 to 8.42775, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4534 - accuracy: 0.3608 - val_loss: 8.4277 - val_accuracy: 0.2096\n",
      "Epoch 19/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4303 - accuracy: 0.3646\n",
      "Epoch 00019: val_loss did not improve from 8.42775\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4272 - accuracy: 0.3651 - val_loss: 8.8489 - val_accuracy: 0.2337\n",
      "Epoch 20/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4426 - accuracy: 0.3577\n",
      "Epoch 00020: val_loss did not improve from 8.42775\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4470 - accuracy: 0.3552 - val_loss: 9.5808 - val_accuracy: 0.1838\n",
      "Epoch 21/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4218 - accuracy: 0.3622\n",
      "Epoch 00021: val_loss did not improve from 8.42775\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4199 - accuracy: 0.3612 - val_loss: 9.3525 - val_accuracy: 0.2165\n",
      "Epoch 22/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4279 - accuracy: 0.3600\n",
      "Epoch 00022: val_loss did not improve from 8.42775\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4245 - accuracy: 0.3599 - val_loss: 9.2922 - val_accuracy: 0.1924\n",
      "Epoch 23/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4413 - accuracy: 0.3568\n",
      "Epoch 00023: val_loss did not improve from 8.42775\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4409 - accuracy: 0.3552 - val_loss: 8.8252 - val_accuracy: 0.1976\n",
      "Epoch 24/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4214 - accuracy: 0.3532\n",
      "Epoch 00024: val_loss did not improve from 8.42775\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4175 - accuracy: 0.3569 - val_loss: 8.8470 - val_accuracy: 0.2199\n",
      "Epoch 25/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4097 - accuracy: 0.3572\n",
      "Epoch 00025: val_loss improved from 8.42775 to 8.24143, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4162 - accuracy: 0.3535 - val_loss: 8.2414 - val_accuracy: 0.2234\n",
      "Epoch 26/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4074 - accuracy: 0.3613\n",
      "Epoch 00026: val_loss did not improve from 8.24143\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.4062 - accuracy: 0.3629 - val_loss: 9.2051 - val_accuracy: 0.2113\n",
      "Epoch 27/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3959 - accuracy: 0.3777\n",
      "Epoch 00027: val_loss improved from 8.24143 to 8.18281, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 30s 2s/step - loss: 1.3952 - accuracy: 0.3784 - val_loss: 8.1828 - val_accuracy: 0.2371\n",
      "Epoch 28/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3897 - accuracy: 0.3827\n",
      "Epoch 00028: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3918 - accuracy: 0.3810 - val_loss: 8.1878 - val_accuracy: 0.1993\n",
      "Epoch 29/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3909 - accuracy: 0.3808\n",
      "Epoch 00029: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3938 - accuracy: 0.3767 - val_loss: 8.1924 - val_accuracy: 0.2457\n",
      "Epoch 30/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3499 - accuracy: 0.3781\n",
      "Epoch 00030: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3536 - accuracy: 0.3797 - val_loss: 8.6510 - val_accuracy: 0.1959\n",
      "Epoch 31/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.4019 - accuracy: 0.3663\n",
      "Epoch 00031: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3914 - accuracy: 0.3681 - val_loss: 8.5616 - val_accuracy: 0.2268\n",
      "Epoch 32/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3547 - accuracy: 0.3867\n",
      "Epoch 00032: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3553 - accuracy: 0.3870 - val_loss: 8.6876 - val_accuracy: 0.2079\n",
      "Epoch 33/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3830 - accuracy: 0.3781\n",
      "Epoch 00033: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3790 - accuracy: 0.3775 - val_loss: 8.2889 - val_accuracy: 0.2096\n",
      "Epoch 34/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3575 - accuracy: 0.3659\n",
      "Epoch 00034: val_loss did not improve from 8.18281\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3598 - accuracy: 0.3655 - val_loss: 8.3017 - val_accuracy: 0.2234\n",
      "Epoch 35/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3463 - accuracy: 0.3890\n",
      "Epoch 00035: val_loss improved from 8.18281 to 7.87249, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3503 - accuracy: 0.3882 - val_loss: 7.8725 - val_accuracy: 0.2234\n",
      "Epoch 36/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3510 - accuracy: 0.3831\n",
      "Epoch 00036: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3465 - accuracy: 0.3857 - val_loss: 8.6231 - val_accuracy: 0.2062\n",
      "Epoch 37/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3839 - accuracy: 0.3668\n",
      "Epoch 00037: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3789 - accuracy: 0.3715 - val_loss: 8.1299 - val_accuracy: 0.2096\n",
      "Epoch 38/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3583 - accuracy: 0.3827\n",
      "Epoch 00038: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3636 - accuracy: 0.3788 - val_loss: 8.8395 - val_accuracy: 0.1976\n",
      "Epoch 39/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3450 - accuracy: 0.3872\n",
      "Epoch 00039: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3484 - accuracy: 0.3857 - val_loss: 8.5079 - val_accuracy: 0.2079\n",
      "Epoch 40/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3683 - accuracy: 0.3854\n",
      "Epoch 00040: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3670 - accuracy: 0.3870 - val_loss: 8.3973 - val_accuracy: 0.2096\n",
      "Epoch 41/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3633 - accuracy: 0.3777\n",
      "Epoch 00041: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3629 - accuracy: 0.3771 - val_loss: 8.3711 - val_accuracy: 0.2165\n",
      "Epoch 42/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3407 - accuracy: 0.3902\n",
      "Epoch 00042: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3394 - accuracy: 0.3895 - val_loss: 8.3029 - val_accuracy: 0.2474\n",
      "Epoch 43/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3504 - accuracy: 0.3949\n",
      "Epoch 00043: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3517 - accuracy: 0.3930 - val_loss: 8.8522 - val_accuracy: 0.2027\n",
      "Epoch 44/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3375 - accuracy: 0.3963\n",
      "Epoch 00044: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3358 - accuracy: 0.3994 - val_loss: 8.5392 - val_accuracy: 0.2182\n",
      "Epoch 45/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3289 - accuracy: 0.4004\n",
      "Epoch 00045: val_loss did not improve from 7.87249\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3293 - accuracy: 0.3981 - val_loss: 8.4834 - val_accuracy: 0.2045\n",
      "Epoch 46/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3450 - accuracy: 0.3881\n",
      "Epoch 00046: val_loss improved from 7.87249 to 7.50438, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3471 - accuracy: 0.3895 - val_loss: 7.5044 - val_accuracy: 0.2182\n",
      "Epoch 47/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3383 - accuracy: 0.3899\n",
      "Epoch 00047: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3380 - accuracy: 0.3908 - val_loss: 8.2637 - val_accuracy: 0.1976\n",
      "Epoch 48/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3338 - accuracy: 0.3958\n",
      "Epoch 00048: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3318 - accuracy: 0.3960 - val_loss: 7.8769 - val_accuracy: 0.1873\n",
      "Epoch 49/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3519 - accuracy: 0.3849\n",
      "Epoch 00049: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3502 - accuracy: 0.3852 - val_loss: 8.3560 - val_accuracy: 0.2045\n",
      "Epoch 50/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3099 - accuracy: 0.4035\n",
      "Epoch 00050: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3092 - accuracy: 0.4037 - val_loss: 8.3582 - val_accuracy: 0.2165\n",
      "Epoch 51/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3159 - accuracy: 0.4085\n",
      "Epoch 00051: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3182 - accuracy: 0.4084 - val_loss: 8.4992 - val_accuracy: 0.2045\n",
      "Epoch 52/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3092 - accuracy: 0.4072\n",
      "Epoch 00052: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3089 - accuracy: 0.4054 - val_loss: 7.9105 - val_accuracy: 0.2182\n",
      "Epoch 53/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3582 - accuracy: 0.3872\n",
      "Epoch 00053: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3522 - accuracy: 0.3870 - val_loss: 8.0608 - val_accuracy: 0.2148\n",
      "Epoch 54/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3318 - accuracy: 0.3858\n",
      "Epoch 00054: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3322 - accuracy: 0.3857 - val_loss: 8.4391 - val_accuracy: 0.2354\n",
      "Epoch 55/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3104 - accuracy: 0.4013\n",
      "Epoch 00055: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3130 - accuracy: 0.3998 - val_loss: 7.7071 - val_accuracy: 0.2148\n",
      "Epoch 56/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3122 - accuracy: 0.4226\n",
      "Epoch 00056: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3036 - accuracy: 0.4264 - val_loss: 8.1006 - val_accuracy: 0.2165\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3152 - accuracy: 0.4040\n",
      "Epoch 00057: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3097 - accuracy: 0.4088 - val_loss: 7.9036 - val_accuracy: 0.2337\n",
      "Epoch 58/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2895 - accuracy: 0.3936\n",
      "Epoch 00058: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2901 - accuracy: 0.3925 - val_loss: 8.1801 - val_accuracy: 0.2302\n",
      "Epoch 59/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3138 - accuracy: 0.4135\n",
      "Epoch 00059: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3243 - accuracy: 0.4088 - val_loss: 7.5926 - val_accuracy: 0.2560\n",
      "Epoch 60/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3225 - accuracy: 0.4062\n",
      "Epoch 00060: val_loss did not improve from 7.50438\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3108 - accuracy: 0.4071 - val_loss: 7.9265 - val_accuracy: 0.2388\n",
      "Epoch 61/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3089 - accuracy: 0.4149\n",
      "Epoch 00061: val_loss improved from 7.50438 to 7.07586, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3097 - accuracy: 0.4148 - val_loss: 7.0759 - val_accuracy: 0.2457\n",
      "Epoch 62/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2903 - accuracy: 0.4149\n",
      "Epoch 00062: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2896 - accuracy: 0.4153 - val_loss: 7.2991 - val_accuracy: 0.2268\n",
      "Epoch 63/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2964 - accuracy: 0.4067\n",
      "Epoch 00063: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2979 - accuracy: 0.4058 - val_loss: 8.5140 - val_accuracy: 0.2131\n",
      "Epoch 64/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2975 - accuracy: 0.4181\n",
      "Epoch 00064: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2986 - accuracy: 0.4196 - val_loss: 7.9399 - val_accuracy: 0.2045\n",
      "Epoch 65/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3154 - accuracy: 0.4194\n",
      "Epoch 00065: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3104 - accuracy: 0.4226 - val_loss: 7.5426 - val_accuracy: 0.2302\n",
      "Epoch 66/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2958 - accuracy: 0.4058\n",
      "Epoch 00066: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2926 - accuracy: 0.4076 - val_loss: 7.4120 - val_accuracy: 0.2182\n",
      "Epoch 67/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3059 - accuracy: 0.4076\n",
      "Epoch 00067: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3014 - accuracy: 0.4084 - val_loss: 8.2179 - val_accuracy: 0.2354\n",
      "Epoch 68/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2942 - accuracy: 0.4067\n",
      "Epoch 00068: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2869 - accuracy: 0.4067 - val_loss: 7.6479 - val_accuracy: 0.1993\n",
      "Epoch 69/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2618 - accuracy: 0.4258\n",
      "Epoch 00069: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2635 - accuracy: 0.4247 - val_loss: 8.0491 - val_accuracy: 0.2199\n",
      "Epoch 70/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3180 - accuracy: 0.4058\n",
      "Epoch 00070: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3164 - accuracy: 0.4050 - val_loss: 7.7100 - val_accuracy: 0.2595\n",
      "Epoch 71/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3057 - accuracy: 0.4099\n",
      "Epoch 00071: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3122 - accuracy: 0.4076 - val_loss: 7.9020 - val_accuracy: 0.2320\n",
      "Epoch 72/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2756 - accuracy: 0.4226\n",
      "Epoch 00072: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2712 - accuracy: 0.4264 - val_loss: 7.6444 - val_accuracy: 0.2113\n",
      "Epoch 73/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2797 - accuracy: 0.4226\n",
      "Epoch 00073: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2834 - accuracy: 0.4213 - val_loss: 7.7926 - val_accuracy: 0.2543\n",
      "Epoch 74/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3024 - accuracy: 0.4008\n",
      "Epoch 00074: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2989 - accuracy: 0.4033 - val_loss: 7.2980 - val_accuracy: 0.2560\n",
      "Epoch 75/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2822 - accuracy: 0.4208\n",
      "Epoch 00075: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2782 - accuracy: 0.4251 - val_loss: 7.5815 - val_accuracy: 0.2526\n",
      "Epoch 76/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2753 - accuracy: 0.4276\n",
      "Epoch 00076: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2768 - accuracy: 0.4281 - val_loss: 8.0880 - val_accuracy: 0.2388\n",
      "Epoch 77/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2862 - accuracy: 0.4117\n",
      "Epoch 00077: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2847 - accuracy: 0.4118 - val_loss: 7.7027 - val_accuracy: 0.2234\n",
      "Epoch 78/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3036 - accuracy: 0.4108\n",
      "Epoch 00078: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.3102 - accuracy: 0.4088 - val_loss: 7.5900 - val_accuracy: 0.2371\n",
      "Epoch 79/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2723 - accuracy: 0.4262\n",
      "Epoch 00079: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2740 - accuracy: 0.4273 - val_loss: 7.6697 - val_accuracy: 0.2509\n",
      "Epoch 80/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2795 - accuracy: 0.4244\n",
      "Epoch 00080: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2796 - accuracy: 0.4251 - val_loss: 7.9279 - val_accuracy: 0.2216\n",
      "Epoch 81/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2753 - accuracy: 0.4235\n",
      "Epoch 00081: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2730 - accuracy: 0.4277 - val_loss: 7.7806 - val_accuracy: 0.2474\n",
      "Epoch 82/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2552 - accuracy: 0.4271\n",
      "Epoch 00082: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2563 - accuracy: 0.4256 - val_loss: 7.4941 - val_accuracy: 0.2268\n",
      "Epoch 83/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2672 - accuracy: 0.4303\n",
      "Epoch 00083: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2661 - accuracy: 0.4290 - val_loss: 7.3604 - val_accuracy: 0.2148\n",
      "Epoch 84/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2558 - accuracy: 0.4290\n",
      "Epoch 00084: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2567 - accuracy: 0.4324 - val_loss: 7.8250 - val_accuracy: 0.2474\n",
      "Epoch 85/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2817 - accuracy: 0.4226\n",
      "Epoch 00085: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2813 - accuracy: 0.4247 - val_loss: 7.4389 - val_accuracy: 0.2234\n",
      "Epoch 86/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2735 - accuracy: 0.4262\n",
      "Epoch 00086: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2723 - accuracy: 0.4251 - val_loss: 7.2233 - val_accuracy: 0.2474\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2542 - accuracy: 0.4303\n",
      "Epoch 00087: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2562 - accuracy: 0.4311 - val_loss: 7.4958 - val_accuracy: 0.2320\n",
      "Epoch 88/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2513 - accuracy: 0.4344\n",
      "Epoch 00088: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2553 - accuracy: 0.4341 - val_loss: 7.0804 - val_accuracy: 0.2302\n",
      "Epoch 89/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2678 - accuracy: 0.4076\n",
      "Epoch 00089: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2680 - accuracy: 0.4127 - val_loss: 7.2311 - val_accuracy: 0.2251\n",
      "Epoch 90/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2575 - accuracy: 0.4330\n",
      "Epoch 00090: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 29s 2s/step - loss: 1.2606 - accuracy: 0.4281 - val_loss: 7.8155 - val_accuracy: 0.2131\n",
      "Epoch 91/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2538 - accuracy: 0.4240\n",
      "Epoch 00091: val_loss did not improve from 7.07586\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2588 - accuracy: 0.4230 - val_loss: 7.1691 - val_accuracy: 0.2491\n",
      "Epoch 00091: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history_inceptionv3_fe_drop_128 = inceptionv3_fe_drop_128.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=200,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint, earlystopping],\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save\n",
    "models.save_model(inceptionv3_fe_drop_128, 'inceptionv3_fe_drop_128_4cl_end.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# History of accuracy and loss\n",
    "tra_loss_fe = history_inceptionv3_fe_drop_128.history['loss']\n",
    "tra_acc_fe = history_inceptionv3_fe_drop_128.history['accuracy']\n",
    "val_loss_fe = history_inceptionv3_fe_drop_128.history['val_loss']\n",
    "val_acc_fe = history_inceptionv3_fe_drop_128.history['val_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n"
     ]
    }
   ],
   "source": [
    "# Total number of epochs training\n",
    "epochs_fe = range(1, len(tra_acc_fe)+1)\n",
    "end_epoch_fe = len(tra_acc_fe)\n",
    "\n",
    "# Epoch when reached the validation loss minimum\n",
    "opt_epoch_fe = val_loss_fe.index(min(val_loss_fe)) + 1\n",
    "\n",
    "# Loss and accuracy on the validation set\n",
    "end_val_loss_fe = val_loss_fe[-1]\n",
    "end_val_acc_fe = val_acc_fe[-1]\n",
    "opt_val_loss_fe = val_loss_fe[opt_epoch_fe-1]\n",
    "opt_val_acc_fe = val_acc_fe[opt_epoch_fe-1]\n",
    "\n",
    "# Loss and accuracy on the test set\n",
    "opt_inceptionv3_fe_drop_128 = models.load_model('inceptionv3_fe_drop_128_4cl_best.h5')\n",
    "test_loss_fe, test_acc_fe = inceptionv3_fe_drop_128.evaluate(test_images, test_labels, verbose=False)\n",
    "opt_test_loss_fe, opt_test_acc_fe = opt_inceptionv3_fe_drop_128.evaluate(test_images, test_labels, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inceptionv3 (w/ dropout, smaller FC) Feature Extraction\n",
      "\n",
      "Epoch [end]: 52\n",
      "Epoch [opt]: 22\n",
      "Valid accuracy [end]: 0.3763\n",
      "Valid accuracy [opt]: 0.3780\n",
      "Test accuracy [end]:  0.3221\n",
      "Test accuracy [opt]:  0.3191\n",
      "Valid loss [end]: 1.5079\n",
      "Valid loss [opt]: 1.3556\n",
      "Test loss [end]:  1.4894\n",
      "Test loss [opt]:  1.3973\n"
     ]
    }
   ],
   "source": [
    "print(\"inceptionv3 (w/ dropout, smaller FC) Feature Extraction\\n\")\n",
    "\n",
    "print(\"Epoch [end]: %d\" % end_epoch_fe)\n",
    "print(\"Epoch [opt]: %d\" % opt_epoch_fe)\n",
    "print(\"Valid accuracy [end]: %.4f\" % end_val_acc_fe)\n",
    "print(\"Valid accuracy [opt]: %.4f\" % opt_val_acc_fe)\n",
    "print(\"Test accuracy [end]:  %.4f\" % test_acc_fe)\n",
    "print(\"Test accuracy [opt]:  %.4f\" % opt_test_acc_fe)\n",
    "print(\"Valid loss [end]: %.4f\" % end_val_loss_fe)\n",
    "print(\"Valid loss [opt]: %.4f\" % opt_val_loss_fe)\n",
    "print(\"Test loss [end]:  %.4f\" % test_loss_fe)\n",
    "print(\"Test loss [opt]:  %.4f\" % opt_test_loss_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 560x560 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 560x560 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model accuracy\n",
    "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title('inceptionv3 w/ dropout FC 128 FE accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(epochs_fe, tra_acc_fe, 'r', label='Training set')\n",
    "plt.plot(epochs_fe, val_acc_fe, 'g', label='Validation set')\n",
    "plt.plot(opt_epoch_fe, val_acc_fe[opt_epoch_fe-1], 'go')\n",
    "plt.vlines(opt_epoch_fe, min(val_acc_fe), opt_val_acc_fe, linestyle=\"dashed\", color='g', linewidth=1)\n",
    "plt.hlines(opt_val_acc_fe, 1, opt_epoch_fe, linestyle=\"dashed\", color='g', linewidth=1)\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "# Model loss\n",
    "plt.figure(figsize=(7, 7), dpi=80, facecolor='w', edgecolor='k')\n",
    "plt.title('inceptionv3 w/ dropout FC 128 FE loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.plot(epochs_fe, tra_loss_fe, 'r', label='Training set')\n",
    "plt.plot(epochs_fe, val_loss_fe, 'g', label='Validation set')\n",
    "plt.plot(opt_epoch_fe, val_loss_fe[opt_epoch_fe-1], 'go')\n",
    "plt.vlines(opt_epoch_fe, min(val_loss_fe), opt_val_loss_fe, linestyle=\"dashed\", color='g', linewidth=1)\n",
    "plt.hlines(opt_val_loss_fe, 1, opt_epoch_fe, linestyle=\"dashed\", color='g', linewidth=1)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=0\n",
    "for layer in inceptionv3_fe_drop_128.get_layer('inception_v3').layers:\n",
    "    x= x+1\n",
    "    layer.trainable = True\n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        # we do aggressive exponential smoothing of batch norm \n",
    "        # parameters to faster adjust to our new dataset\n",
    "        layer.momentum = 0.8\n",
    "    \n",
    "# fix deep layers (fine-tuning only last 50)\n",
    "for layer in inceptionv3_fe_drop_128.get_layer('inception_v3').layers[:-50]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "311"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping (stop training after the validation loss reaches the minimum)\n",
    "earlystopping = EarlyStopping(monitor='val_loss', mode='min', patience=30, verbose=1)\n",
    "\n",
    "# Callback for checkpointing\n",
    "checkpoint = ModelCheckpoint('inceptionv3_fe_drop_128_4cl_best.h5', \n",
    "        monitor='val_loss', mode='min', verbose=1, \n",
    "        save_best_only=True, save_freq='epoch'\n",
    ")\n",
    "\n",
    "# Compile the model\n",
    "inceptionv3_fe_drop_128.compile(optimizer=RMSprop(lr=0.001, rho=0.9),loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 19 steps, validate for 5 steps\n",
      "Epoch 1/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 14.3664 - accuracy: 0.2851\n",
      "Epoch 00001: val_loss improved from inf to 5.25294, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 33s 2s/step - loss: 13.6833 - accuracy: 0.2891 - val_loss: 5.2529 - val_accuracy: 0.2371\n",
      "Epoch 2/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3637 - accuracy: 0.3677\n",
      "Epoch 00002: val_loss improved from 5.25294 to 1.41222, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3657 - accuracy: 0.3719 - val_loss: 1.4122 - val_accuracy: 0.3557\n",
      "Epoch 3/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3737 - accuracy: 0.3640\n",
      "Epoch 00003: val_loss improved from 1.41222 to 1.38028, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3746 - accuracy: 0.3625 - val_loss: 1.3803 - val_accuracy: 0.3763\n",
      "Epoch 4/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3890 - accuracy: 0.3273\n",
      "Epoch 00004: val_loss improved from 1.38028 to 1.37745, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3883 - accuracy: 0.3303 - val_loss: 1.3775 - val_accuracy: 0.3763\n",
      "Epoch 5/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3752 - accuracy: 0.3436\n",
      "Epoch 00005: val_loss did not improve from 1.37745\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3752 - accuracy: 0.3471 - val_loss: 7.0288 - val_accuracy: 0.2371\n",
      "Epoch 6/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3518 - accuracy: 0.3995\n",
      "Epoch 00006: val_loss did not improve from 1.37745\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3512 - accuracy: 0.3994 - val_loss: 2.4366 - val_accuracy: 0.2646\n",
      "Epoch 7/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3230 - accuracy: 0.4208\n",
      "Epoch 00007: val_loss improved from 1.37745 to 1.36995, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3265 - accuracy: 0.4161 - val_loss: 1.3699 - val_accuracy: 0.3746\n",
      "Epoch 8/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3188 - accuracy: 0.4326\n",
      "Epoch 00008: val_loss did not improve from 1.36995\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3181 - accuracy: 0.4329 - val_loss: 1.5550 - val_accuracy: 0.3333\n",
      "Epoch 9/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.3024 - accuracy: 0.4367\n",
      "Epoch 00009: val_loss did not improve from 1.36995\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.3003 - accuracy: 0.4402 - val_loss: 1.5456 - val_accuracy: 0.3282\n",
      "Epoch 10/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2828 - accuracy: 0.4353\n",
      "Epoch 00010: val_loss did not improve from 1.36995\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2806 - accuracy: 0.4350 - val_loss: 1.4287 - val_accuracy: 0.3625\n",
      "Epoch 11/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2757 - accuracy: 0.4518\n",
      "Epoch 00011: val_loss improved from 1.36995 to 1.36258, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2793 - accuracy: 0.4500 - val_loss: 1.3626 - val_accuracy: 0.3763\n",
      "Epoch 12/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2762 - accuracy: 0.4471\n",
      "Epoch 00012: val_loss did not improve from 1.36258\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2752 - accuracy: 0.4517 - val_loss: 1.3661 - val_accuracy: 0.3729\n",
      "Epoch 13/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2604 - accuracy: 0.4594\n",
      "Epoch 00013: val_loss did not improve from 1.36258\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2590 - accuracy: 0.4603 - val_loss: 1.5024 - val_accuracy: 0.3660\n",
      "Epoch 14/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2584 - accuracy: 0.4585\n",
      "Epoch 00014: val_loss improved from 1.36258 to 1.35633, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2572 - accuracy: 0.4543 - val_loss: 1.3563 - val_accuracy: 0.3763\n",
      "Epoch 15/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2497 - accuracy: 0.4580\n",
      "Epoch 00015: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2501 - accuracy: 0.4543 - val_loss: 1.3787 - val_accuracy: 0.3814\n",
      "Epoch 16/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2707 - accuracy: 0.4417\n",
      "Epoch 00016: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2685 - accuracy: 0.4406 - val_loss: 1.4041 - val_accuracy: 0.3780\n",
      "Epoch 17/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2404 - accuracy: 0.4592\n",
      "Epoch 00017: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2397 - accuracy: 0.4582 - val_loss: 1.3574 - val_accuracy: 0.3763\n",
      "Epoch 18/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2420 - accuracy: 0.4580\n",
      "Epoch 00018: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2414 - accuracy: 0.4582 - val_loss: 1.3730 - val_accuracy: 0.3780\n",
      "Epoch 19/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2319 - accuracy: 0.4471\n",
      "Epoch 00019: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2301 - accuracy: 0.4543 - val_loss: 1.4001 - val_accuracy: 0.3832\n",
      "Epoch 20/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2273 - accuracy: 0.4557\n",
      "Epoch 00020: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2272 - accuracy: 0.4539 - val_loss: 1.8625 - val_accuracy: 0.3660\n",
      "Epoch 21/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2326 - accuracy: 0.4485\n",
      "Epoch 00021: val_loss did not improve from 1.35633\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2326 - accuracy: 0.4479 - val_loss: 1.3954 - val_accuracy: 0.3677\n",
      "Epoch 22/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2227 - accuracy: 0.4480\n",
      "Epoch 00022: val_loss improved from 1.35633 to 1.35564, saving model to inceptionv3_fe_drop_128_4cl_best.h5\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2223 - accuracy: 0.4535 - val_loss: 1.3556 - val_accuracy: 0.3780\n",
      "Epoch 23/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2020 - accuracy: 0.4662\n",
      "Epoch 00023: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1959 - accuracy: 0.4715 - val_loss: 2.3922 - val_accuracy: 0.3058\n",
      "Epoch 24/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2298 - accuracy: 0.4603\n",
      "Epoch 00024: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2328 - accuracy: 0.4556 - val_loss: 1.3661 - val_accuracy: 0.3746\n",
      "Epoch 25/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2042 - accuracy: 0.4603\n",
      "Epoch 00025: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2082 - accuracy: 0.4582 - val_loss: 4.5699 - val_accuracy: 0.2216\n",
      "Epoch 26/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2289 - accuracy: 0.4498\n",
      "Epoch 00026: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2255 - accuracy: 0.4513 - val_loss: 1.5502 - val_accuracy: 0.3763\n",
      "Epoch 27/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1987 - accuracy: 0.4671\n",
      "Epoch 00027: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1988 - accuracy: 0.4689 - val_loss: 1.3636 - val_accuracy: 0.3797\n",
      "Epoch 28/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1966 - accuracy: 0.4666\n",
      "Epoch 00028: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1949 - accuracy: 0.4680 - val_loss: 1.3791 - val_accuracy: 0.3797\n",
      "Epoch 29/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1768 - accuracy: 0.4694\n",
      "Epoch 00029: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1770 - accuracy: 0.4685 - val_loss: 1.3653 - val_accuracy: 0.3763\n",
      "Epoch 30/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.2099 - accuracy: 0.4621\n",
      "Epoch 00030: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.2063 - accuracy: 0.4655 - val_loss: 1.5525 - val_accuracy: 0.3677\n",
      "Epoch 31/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1938 - accuracy: 0.4553\n",
      "Epoch 00031: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1896 - accuracy: 0.4590 - val_loss: 1.5241 - val_accuracy: 0.3780\n",
      "Epoch 32/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1841 - accuracy: 0.4626\n",
      "Epoch 00032: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1812 - accuracy: 0.4642 - val_loss: 1.6070 - val_accuracy: 0.3677\n",
      "Epoch 33/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1937 - accuracy: 0.4598\n",
      "Epoch 00033: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1945 - accuracy: 0.4590 - val_loss: 1.4720 - val_accuracy: 0.3729\n",
      "Epoch 34/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1902 - accuracy: 0.4725\n",
      "Epoch 00034: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1878 - accuracy: 0.4732 - val_loss: 1.4571 - val_accuracy: 0.3660\n",
      "Epoch 35/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1646 - accuracy: 0.4725\n",
      "Epoch 00035: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1629 - accuracy: 0.4736 - val_loss: 1.6891 - val_accuracy: 0.3677\n",
      "Epoch 36/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1724 - accuracy: 0.4721\n",
      "Epoch 00036: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1702 - accuracy: 0.4740 - val_loss: 1.6357 - val_accuracy: 0.3746\n",
      "Epoch 37/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1908 - accuracy: 0.4585\n",
      "Epoch 00037: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1851 - accuracy: 0.4633 - val_loss: 1.6112 - val_accuracy: 0.3780\n",
      "Epoch 38/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1700 - accuracy: 0.4680\n",
      "Epoch 00038: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1674 - accuracy: 0.4719 - val_loss: 1.3854 - val_accuracy: 0.3763\n",
      "Epoch 39/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1696 - accuracy: 0.4798\n",
      "Epoch 00039: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1727 - accuracy: 0.4766 - val_loss: 1.4045 - val_accuracy: 0.3763\n",
      "Epoch 40/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1762 - accuracy: 0.4712\n",
      "Epoch 00040: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1749 - accuracy: 0.4728 - val_loss: 1.5377 - val_accuracy: 0.3729\n",
      "Epoch 41/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1658 - accuracy: 0.4607\n",
      "Epoch 00041: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1702 - accuracy: 0.4633 - val_loss: 1.4786 - val_accuracy: 0.3780\n",
      "Epoch 42/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1668 - accuracy: 0.4616\n",
      "Epoch 00042: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1667 - accuracy: 0.4629 - val_loss: 1.9447 - val_accuracy: 0.3883\n",
      "Epoch 43/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1680 - accuracy: 0.4644\n",
      "Epoch 00043: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1667 - accuracy: 0.4650 - val_loss: 1.5426 - val_accuracy: 0.3763\n",
      "Epoch 44/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1534 - accuracy: 0.4818\n",
      "Epoch 00044: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1543 - accuracy: 0.4809 - val_loss: 1.5126 - val_accuracy: 0.3763\n",
      "Epoch 45/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1623 - accuracy: 0.4648\n",
      "Epoch 00045: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1609 - accuracy: 0.4663 - val_loss: 1.5518 - val_accuracy: 0.3814\n",
      "Epoch 46/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1694 - accuracy: 0.4639\n",
      "Epoch 00046: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1704 - accuracy: 0.4629 - val_loss: 1.4661 - val_accuracy: 0.3729\n",
      "Epoch 47/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1579 - accuracy: 0.4675\n",
      "Epoch 00047: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1586 - accuracy: 0.4663 - val_loss: 1.4194 - val_accuracy: 0.3746\n",
      "Epoch 48/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1597 - accuracy: 0.4553\n",
      "Epoch 00048: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1584 - accuracy: 0.4582 - val_loss: 2.2269 - val_accuracy: 0.3711\n",
      "Epoch 49/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1496 - accuracy: 0.4830\n",
      "Epoch 00049: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1509 - accuracy: 0.4796 - val_loss: 2.2868 - val_accuracy: 0.3522\n",
      "Epoch 50/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1591 - accuracy: 0.4762\n",
      "Epoch 00050: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1567 - accuracy: 0.4749 - val_loss: 1.7343 - val_accuracy: 0.3660\n",
      "Epoch 51/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1557 - accuracy: 0.4744\n",
      "Epoch 00051: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1553 - accuracy: 0.4723 - val_loss: 1.7327 - val_accuracy: 0.3643\n",
      "Epoch 52/200\n",
      "18/19 [===========================>..] - ETA: 1s - loss: 1.1280 - accuracy: 0.4848\n",
      "Epoch 00052: val_loss did not improve from 1.35564\n",
      "19/19 [==============================] - 30s 2s/step - loss: 1.1369 - accuracy: 0.4831 - val_loss: 1.5079 - val_accuracy: 0.3763\n",
      "Epoch 00052: early stopping\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "history_inceptionv3_fe_drop_128 = inceptionv3_fe_drop_128.fit_generator(\n",
    "        train_generator,\n",
    "        epochs=200,\n",
    "        validation_data=validation_generator,\n",
    "        callbacks=[checkpoint, earlystopping],\n",
    "        shuffle=True,\n",
    "        verbose=1,\n",
    "        initial_epoch=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
